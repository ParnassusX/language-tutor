// server.js
const express = require('express');
const http = require('http');
const WebSocket = require('ws');
const cors = require('cors');
require('dotenv').config();

const app = express();
const server = http.createServer(app);
const wss = new WebSocket.Server({ server });

app.use(cors());
app.use(express.json());

// Configurazione servizi vocali
const DEEPGRAM_API_KEY = process.env.DEEPGRAM_API_KEY;
const ELEVENLABS_API_KEY = process.env.ELEVENLABS_API_KEY;
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;

// WebSocket handler per gestire le connessioni audio
wss.on('connection', (ws) => {
    console.log('Nuova connessione WebSocket stabilita');
    
    let deepgramSocket = null;
    let conversationContext = [];
    
    // Inizializza connessione Deepgram per STT
    const initializeDeepgram = () => {
        const deepgramUrl = `wss://api.deepgram.com/v1/listen?encoding=linear16&sample_rate=16000&language=it&model=nova-2&smart_format=true`;
        
        deepgramSocket = new WebSocket(deepgramUrl, {
            headers: {
                Authorization: `Token ${DEEPGRAM_API_KEY}`,
            },
        });
        
        deepgramSocket.on('open', () => {
            console.log('Connessione Deepgram stabilita');
        });
        
        deepgramSocket.on('message', async (data) => {
            const response = JSON.parse(data);
            
            if (response.channel?.alternatives[0]?.transcript) {
                const transcript = response.channel.alternatives[0].transcript;
                
                if (transcript && response.is_final) {
                    console.log('Trascrizione finale:', transcript);
                    
                    // Processa con AI
                    const aiResponse = await processWithAI(transcript, conversationContext);
                    
                    // Sintetizza risposta
                    const audioData = await synthesizeSpeech(aiResponse);
                    
                    // Invia audio al client
                    ws.send(JSON.stringify({
                        type: 'audio',
                        data: audioData,
                        text: aiResponse
                    }));
                    
                    // Aggiorna contesto conversazione
                    conversationContext.push(
                        { role: 'user', content: transcript },
                        { role: 'assistant', content: aiResponse }
                    );
                }
            }
        });
    };
    
    // Gestione messaggi dal client
    ws.on('message', (message) => {
        if (message instanceof Buffer) {
            // Audio data dal client
            if (deepgramSocket && deepgramSocket.readyState === WebSocket.OPEN) {
                deepgramSocket.send(message);
            }
        } else {
            // Comandi di controllo
            const data = JSON.parse(message);
            
            if (data.type === 'start') {
                initializeDeepgram();
            } else if (data.type === 'stop') {
                if (deepgramSocket) {
                    deepgramSocket.close();
                }
            }
        }
    });
    
    ws.on('close', () => {
        console.log('Connessione WebSocket chiusa');
        if (deepgramSocket) {
            deepgramSocket.close();
        }
    });
});

// Funzione per processare con AI (OpenAI)
async function processWithAI(text, context) {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
            'Authorization': `Bearer ${OPENAI_API_KEY}`,
            'Content-Type': 'application/json',
        },
        body: JSON.stringify({
            model: 'gpt-4',
            messages: [
                {
                    role: 'system',
                    content: 'Sei un assistente vocale amichevole e professionale. Rispondi in modo conciso e naturale.'
                },
                ...context.slice(-6), // Ultimi 3 scambi
                { role: 'user', content: text }
            ],
            temperature: 0.7,
            max_tokens: 150
        })
    });
    
    const data = await response.json();
    return data.choices[0].message.content;
}

// Funzione per sintesi vocale (ElevenLabs)
async function synthesizeSpeech(text) {
    const voiceId = 'MF3mGyEYCl7XYWbV9V6O'; // Voice ID di ElevenLabs
    
    const response = await fetch(`https://api.elevenlabs.io/v1/text-to-speech/${voiceId}`, {
        method: 'POST',
        headers: {
            'xi-api-key': ELEVENLABS_API_KEY,
            'Content-Type': 'application/json',
        },
        body: JSON.stringify({
            text: text,
            model_id: 'eleven_multilingual_v2',
            voice_settings: {
                stability: 0.5,
                similarity_boost: 0.75
            }
        })
    });
    
    const audioBuffer = await response.arrayBuffer();
    return Buffer.from(audioBuffer).toString('base64');
}

const PORT = process.env.PORT || 3001;
server.listen(PORT, () => {
    console.log(`Server in ascolto sulla porta ${PORT}`);
});
