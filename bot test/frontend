// App.js
import React, { useState, useRef, useEffect } from 'react';
import './App.css';

function App() {
    const [isRecording, setIsRecording] = useState(false);
    const [messages, setMessages] = useState([]);
    const [connectionStatus, setConnectionStatus] = useState('disconnected');
    
    const wsRef = useRef(null);
    const mediaRecorderRef = useRef(null);
    const audioContextRef = useRef(null);
    const audioQueueRef = useRef([]);
    const isPlayingRef = useRef(false);
    
    useEffect(() => {
        // Inizializza WebSocket
        wsRef.current = new WebSocket('ws://localhost:3001');
        
        wsRef.current.onopen = () => {
            setConnectionStatus('connected');
            console.log('Connesso al server');
        };
        
        wsRef.current.onmessage = async (event) => {
            const data = JSON.parse(event.data);
            
            if (data.type === 'audio') {
                // Aggiungi messaggio alla UI
                setMessages(prev => [...prev, {
                    type: 'assistant',
                    text: data.text,
                    timestamp: new Date()
                }]);
                
                // Riproduci audio
                await playAudio(data.data);
            }
        };
        
        wsRef.current.onclose = () => {
            setConnectionStatus('disconnected');
        };
        
        return () => {
            if (wsRef.current) {
                wsRef.current.close();
            }
        };
    }, []);
    
    const startRecording = async () => {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            
            // Crea AudioContext per processare l'audio
            audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({
                sampleRate: 16000
            });
            
            const source = audioContextRef.current.createMediaStreamSource(stream);
            const processor = audioContextRef.current.createScriptProcessor(4096, 1, 1);
            
            processor.onaudioprocess = (e) => {
                if (wsRef.current?.readyState === WebSocket.OPEN) {
                    const inputData = e.inputBuffer.getChannelData(0);
                    const buffer = new ArrayBuffer(inputData.length * 2);
                    const view = new DataView(buffer);
                    
                    for (let i = 0; i < inputData.length; i++) {
                        const s = Math.max(-1, Math.min(1, inputData[i]));
                        view.setInt16(i * 2, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
                    }
                    
                    wsRef.current.send(buffer);
                }
            };
            
            source.connect(processor);
            processor.connect(audioContextRef.current.destination);
            
            // Invia comando di start
            wsRef.current.send(JSON.stringify({ type: 'start' }));
            
            setIsRecording(true);
        } catch (error) {
            console.error('Errore nell\'accesso al microfono:', error);
        }
    };
    
    const stopRecording = () => {
        if (audioContextRef.current) {
            audioContextRef.current.close();
        }
        
        if (wsRef.current?.readyState === WebSocket.OPEN) {
            wsRef.current.send(JSON.stringify({ type: 'stop' }));
        }
        
        setIsRecording(false);
    };
    
    const playAudio = async (base64Audio) => {
        const audioData = atob(base64Audio);
        const arrayBuffer = new ArrayBuffer(audioData.length);
        const view = new Uint8Array(arrayBuffer);
        
        for (let i = 0; i < audioData.length; i++) {
            view[i] = audioData.charCodeAt(i);
        }
        
        const audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
        
        const source = audioContext.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(audioContext.destination);
        source.start();
    };
    
    return (
        <div className="App">
            <header className="App-header">
                <h1>Assistente Vocale AI</h1>
                <div className="status">
                    Stato: <span className={connectionStatus}>{connectionStatus}</span>
                </div>
            </header>
            
            <main className="chat-container">
                <div className="messages">
                    {messages.map((msg, index) => (
                        <div key={index} className={`message ${msg.type}`}>
                            <div className="message-content">{msg.text}</div>
                            <div className="message-time">
                                {msg.timestamp.toLocaleTimeString()}
                            </div>
                        </div>
                    ))}
                </div>
                
                <div className="controls">
                    <button
                        className={`record-button ${isRecording ? 'recording' : ''}`}
                        onClick={isRecording ? stopRecording : startRecording}
                    >
                        {isRecording ? '‚èπÔ∏è Stop' : 'üé§ Parla'}
                    </button>
                </div>
            </main>
        </div>
    );
}

export default App;
